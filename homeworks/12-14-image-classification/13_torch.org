Training linear models and neural networks using torch

The goal is to implement a stochastic gradient descent algorithm for
training a linear model and a neural network. This week we will use
the torch module in python, which provides automatic differentiation
(you don't have to code the gradients yourself, which helps a lot for
the complex gradients in neural networks). You should implement three
different classes.

** Class: TorchModelLinear

- This class implements a predictive model using the torch framework.
- similar to
  https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html#creating-models
- Inherits from torch.nn.Module.
- Defines an __init__ method which creates a linear layer.
- Defines forward method which takes an input matrix (nrow=number of
  observations, ncol=number of features) and returns a matrix
  (nrow=number of observations, ncol=number of classes) with the
  outputs/predicted scores from the neural network.

** Class: TorchLearnerLinear

This class implements fit/predict methods for weight matrix parameter
learning and making predictions with the previous class. 

- __init__ method should store hyper-parameters, max_epochs,
  batch_size, step_size. Also instantiate a
  TorchModel() and save as an attribute of this instance,
  self.model. Also instantiate torch.optim.SGD and save as
  self.optimizer, and instantiate torch.nn.CrossEntropyLoss and save
  as self.loss_fun.
- take_step(batch_features, batch_labels) method should
  - begin by computing self.model(batch_features) and saving as vector of
    predictions for this batch.
  - Use self.loss_fun to compute the mean loss.
  - Use optimizer.zero_grad, loss.backward, optimizer.step to compute
    gradients and take a step as in
    https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html#optimizing-the-model-parameters
- fit(subtrain_data, validation_data=None) method should run gradient
  descent until max_epochs is reached (compute gradients using only
  the subtrain set). There should be two for loops, first over epochs,
  then over batches. You should use the take_step method on each
  batch. Compute and store the subtrain/validation loss at the end of
  each epoch.
- decision_function(test_features) method should return a tensor of
  real number predicted scores given the current weights in the neural
  network. This should have two dimensions (rows=number of
  observations/images, columns=number of classes=10).
- predict(test_features) method should return a tensor of predicted
  classes given the current weights in the neural network. Predict
  whichever class has the highest score returned by decision_function,
  use pred_tensor.argmax(dim=1).

** Hyper-parameter training and diagnostic plot

You should compute the subtrain/validation loss at the end of each
epoch. Run it on the zip data set, and make a plot of
subtrain/validation loss as a function of number of epochs. For full
credit your subtrain loss should always decrease, and your validation
loss should show the expected U shape (if it does not, then you may
need to change hyper-parameters). In the plot, what is the best
number of epochs? (that minimizes validation loss)

Please submit a PDF with your code, results, answer to questions, and
figure with subtrain/validation loss curves.

** Extra credit

- Create TorchModelNN and TorchLearnerNN classes, similar to above,
  but with a neural network instead of a linear model. You need to use
  more than one Linear layer, and there needs to be a non-linear
  activation (ReLU) between linear layers (but no ReLU at the end).

** FAQ

- How to make sure hyper-parameters are correctly chosen? You need to
  experiment with hyper-parameters until you find some combination
  (max_epochs, batch_size, step_size) which results
  in the characteristic loss curves (subtrain mostly always
  decreasing, validation U shaped as number of epochs increases).
