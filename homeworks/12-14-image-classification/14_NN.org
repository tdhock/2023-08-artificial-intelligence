** Experiments/application

- Like last week, use 3-fold CV to define train/test splits.
- Run experiments on zip data.
- Add KNeighborsClassifier + GridSearchCV as another learning
  algorithm. Make sure to search over neighbors from 1 to 20.
- Show a table of resulting test accuracy numbers, as well as a ggplot
  similar to the slides.
- Are your learners significantly more acurate than the featureless
  baseline?

The resulting image should look like:

[[file:figure-example-test-accuracy.png]]

** Extra credit

- For the linear model, plot the subtrain/validation loss/accuracy as a
  function of the regularization hyper-parameter (alpha, degree of L1
  regularization).
- For the nearest neighbors, plot the subtrain/validation loss/accuracy as a
  function of the number of neighbors.

The correct answer should look something like below,

[[file:figure-example-subtrain-validation-loss.png]]
